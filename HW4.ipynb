{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "65e6dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shenxuanyu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/shenxuanyu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shenxuanyu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/shenxuanyu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shenxuanyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#1.\n",
    "df = pd.read_csv (r'/Users/shenxuanyu/Desktop/corona_fake.csv')\n",
    "#print (df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "9ddd8c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You just need to add water, and the drugs and vaccines are ready to be administered. There are two parts to the kit: one holds pellets containing the chemical machinery that synthesises the end product, and the other holds pellets containing instructions that telll the drug which compound to create. Mix two parts together in a chosen combination, add water, and the treatment is ready.\n"
     ]
    }
   ],
   "source": [
    "text = df['text'].astype(str)\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "af347b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list = []\n",
    "for e in text:\n",
    "    e = str(e)\n",
    "    list.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "4a89a74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for e in text:\n",
    "    m = nltk.word_tokenize(e)\n",
    "    tokens.append(m)\n",
    "type(tokens[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "f680b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = []\n",
    "for e in tokens:\n",
    "    #e = ' '.join(e)\n",
    "    m = nltk.pos_tag(e)\n",
    "    tagged.append(m)\n",
    "    \n",
    "WNL = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "22544691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word): #referrence: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "99e2474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_list = []\n",
    "for e in tokens:\n",
    "    lemma_ele = []\n",
    "    for x in e:\n",
    "        lemma = WNL.lemmatize(x, get_wordnet_pos(x))\n",
    "        lemma_ele.append(lemma)\n",
    "    lemma_list.append(lemma_ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "ab986d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'just', 'need', 'to', 'add', 'water', ',', 'and', 'the', 'drug', 'and', 'vaccine', 'be', 'ready', 'to', 'be', 'administer', '.', 'There', 'be', 'two', 'part', 'to', 'the', 'kit', ':', 'one', 'hold', 'pellet', 'contain', 'the', 'chemical', 'machinery', 'that', 'synthesis', 'the', 'end', 'product', ',', 'and', 'the', 'other', 'hold', 'pellet', 'contain', 'instruction', 'that', 'telll', 'the', 'drug', 'which', 'compound', 'to', 'create', '.', 'Mix', 'two', 'part', 'together', 'in', 'a', 'chosen', 'combination', ',', 'add', 'water', ',', 'and', 'the', 'treatment', 'be', 'ready', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemma_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "ab01bf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "36d151e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords_list = []\n",
    "for ele in lemma_list:\n",
    "    list = []\n",
    "    for x in ele:\n",
    "        if x not in stopwords: \n",
    "            list.append(x)\n",
    "    no_stopwords_list.append(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "9d286ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'need', 'add', 'water', ',', 'drug', 'vaccine', 'ready', 'administer', '.', 'There', 'two', 'part', 'kit', ':', 'one', 'hold', 'pellet', 'contain', 'chemical', 'machinery', 'synthesis', 'end', 'product', ',', 'hold', 'pellet', 'contain', 'instruction', 'telll', 'drug', 'compound', 'create', '.', 'Mix', 'two', 'part', 'together', 'chosen', 'combination', ',', 'add', 'water', ',', 'treatment', 'ready', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(no_stopwords_list[0])\n",
    "len(no_stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "5d7697a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list = []\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "url = 'http'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "767635f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_has_emoji(text):\n",
    "    for ele in text:\n",
    "        if ele in emoji.UNICODE_EMOJI:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "7ef8927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ele in no_stopwords_list:#get cleaned text\n",
    "    list = []\n",
    "    for x in ele:\n",
    "        try:\n",
    "            float(x)\n",
    "        except ValueError:\n",
    "            if x.isdigit() == True:\n",
    "                continue\n",
    "            elif len(x) <= 2:\n",
    "                continue\n",
    "            elif x in punc:\n",
    "                continue\n",
    "            elif x in url:\n",
    "                continue\n",
    "            elif text_has_emoji(x) == True:\n",
    "                continue\n",
    "            else:\n",
    "                list.append(x)\n",
    "    final_list.append(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "cbb4f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []#join \n",
    "for ele in final_list:\n",
    "    ele = ' '.join(ele)\n",
    "    final.append(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "cd70f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72385dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "a0df63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "cvectorizer1 = CountVectorizer(ngram_range=(1, 1), lowercase=True)\n",
    "cvectorizer2 = CountVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "cvectorizer3 = CountVectorizer(ngram_range=(1, 3), lowercase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "6baf3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text_clean']\n",
    "X1 = cvectorizer1.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "91e021d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = cvectorizer2.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "5dd47166",
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = cvectorizer3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "f5c7d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvectorizer1 = TfidfVectorizer(ngram_range=(1, 1), lowercase=True)\n",
    "tvectorizer2 = TfidfVectorizer(ngram_range=(1, 2), lowercase=True)\n",
    "tvectorizer3 = TfidfVectorizer(ngram_range=(1, 3), lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "46667014",
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = tvectorizer1.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "852830ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = tvectorizer2.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "75d1df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X6 = tvectorizer3.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "b7fd97ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.\n",
    "logi = LogisticRegressionCV(cv=5, random_state=265, max_iter = 1000, n_jobs = -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "8f959486",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "b7aa0acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9080459770114943"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y, test_size=0.3, random_state=265)\n",
    "logi.fit(X1_train,Y1_train)\n",
    "Y1_pred = logi.predict(X1_test)\n",
    "logi.score(X1_test, Y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "d3a19464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9195402298850575"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y, test_size=0.3, random_state=265)\n",
    "logi.fit(X2_train,Y2_train)\n",
    "Y2_pred = logi.predict(X2_test)\n",
    "logi.score(X2_test, Y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "a3c5688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8994252873563219"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y, test_size=0.3, random_state=265)\n",
    "logi.fit(X3_train,Y3_train)\n",
    "Y3_pred = logi.predict(X3_test)\n",
    "logi.score(X3_test, Y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "3bd12ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9195402298850575"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X4_train, X4_test, Y4_train, Y4_test = train_test_split(X4, Y, test_size=0.3, random_state=265)\n",
    "logi.fit(X4_train,Y4_train)\n",
    "Y4_pred = logi.predict(X4_test)\n",
    "logi.score(X4_test, Y4_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "91820faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9051724137931034"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X5_train, X5_test, Y5_train, Y5_test = train_test_split(X5, Y, test_size=0.3, random_state=265)\n",
    "logi.fit(X5_train,Y5_train)\n",
    "Y5_pred = logi.predict(X5_test)\n",
    "logi.score(X5_test, Y5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "ce17b9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896551724137931"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X6_train, X6_test, Y6_train, Y6_test = train_test_split(X6, Y, test_size=0.3, random_state=265)\n",
    "logi.fit(X6_train,Y6_train)\n",
    "Y6_pred = logi.predict(X6_test)\n",
    "logi.score(X6_test, Y6_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
